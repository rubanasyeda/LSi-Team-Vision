{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "323849c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T06:05:02.684851Z",
     "start_time": "2025-04-29T06:04:32.414999Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/x3/rrwz2xqn1j7fgsvk_zwlgm1c0000gn/T/ipykernel_65673/3031525044.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mcopy\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mdeepcopy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mdc\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mDataset\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnn\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnn\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy as dc\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from geopy.geocoders import Nominatim\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import datetime\n",
    "# from torch.optim import Adam,AdamW\n",
    "# from torch.optim import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab114c4",
   "metadata": {},
   "source": [
    "Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323d4445",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to Toronto Data\n",
    "toronto_path = [\"../dataset/toronto-occupancy*.csv\", \n",
    "                \"../dataset/toronto-weather*.csv\", \n",
    "                \"../dataset/toronto-inflation*.csv\", \n",
    "                \"../dataset/toronto-unemployment*.csv\",\n",
    "                \"../dataset/toronto-cpi*.csv\"]\n",
    "\n",
    "calgary_path = [\"../dataset/calgary-occupancy*.csv\", \n",
    "                \"../dataset/calgary-weather*.csv\", \n",
    "                \"../dataset/calgary-inflation*.csv\", \n",
    "                \"../dataset/calgary-unemployment*.csv\",\n",
    "                \"../dataset/calgary-cpi*.csv\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7050308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_to_pandas(file_path):\n",
    "    try:\n",
    "        # Load CSV file into a pandas data_23Frame\n",
    "        df = pd.read_csv(file_path, header=0, low_memory=False, encoding='unicode_escape')\n",
    "        print(\"Number of rows in the dataFrame:\", file_path, len(df))\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{file_path}' not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "        return None\n",
    "\n",
    "def clean_and_standardize_date(date_str):\n",
    "    try:\n",
    "        # Case 1: Format like 22-01-01 (YY-MM-DD)\n",
    "        if len(date_str) == 8 and '-' in date_str:\n",
    "            return datetime.strptime(date_str, \"%y-%m-%d\").date()\n",
    "        # if len(date_str) == 6 and '-' in date_str:\n",
    "        #     return datetime.strptime(date_str, \"%y-%m\").date()\n",
    "        # Case 2: ISO format with time or standard YYYY-MM-DD\n",
    "        return pd.to_datetime(date_str).date()\n",
    "    \n",
    "    except Exception:\n",
    "        return pd.NaT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08336027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def loadData(city):\n",
    "\n",
    "    city_paths = {\n",
    "    \"calgary\": calgary_path,\n",
    "    \"toronto\": toronto_path,\n",
    "    }\n",
    "    path = city_paths[city]\n",
    "\n",
    "    output_data = glob.glob(path[0])\n",
    "    weather_data = glob.glob(path[1])\n",
    "    inflation = glob.glob(path[2])[0]\n",
    "    unemployment = glob.glob(path[3])[0]\n",
    "    cpi = glob.glob(path[4])[0]\n",
    "\n",
    "    #-------Output Data-------#\n",
    "    #Loading up the links to the output dataset\n",
    "    for i in range(len(output_data)):\n",
    "        output_data[i] = load_csv_to_pandas(output_data[i])\n",
    "\n",
    "    #Dropping irrelevant columns for output datasets\n",
    "    for i in range(len(output_data)):\n",
    "        #print(output_data[i])\n",
    "        output_data[i] = output_data[i].drop(columns = ['_id', 'SHELTER_ID', 'LOCATION_ID', 'LOCATION_CITY', 'LOCATION_PROVINCE', 'PROGRAM_NAME', 'PROGRAM_AREA', 'SERVICE_USER_COUNT', 'CAPACITY_FUNDING_BED', 'UNOCCUPIED_BEDS', 'UNAVAILABLE_BEDS', 'CAPACITY_FUNDING_ROOM', 'UNOCCUPIED_ROOMS', 'UNAVAILABLE_ROOMS'])\n",
    "        output_data[i]['OCCUPANCY_DATE'] = output_data[i]['OCCUPANCY_DATE'].astype(str).apply(clean_and_standardize_date)\n",
    "        output_data[i]['OCCUPANCY_DATE'] =  pd.to_datetime(output_data[i]['OCCUPANCY_DATE'], format='%Y-%m-%d')\n",
    "\n",
    "    #Joining the Output data together\n",
    "    big_data = output_data[0]\n",
    "    for i in range(1,len(output_data)):\n",
    "        big_data = pd.concat([big_data, output_data[i]], ignore_index = True)\n",
    "\n",
    "    #Determine the max and min date in the dataset to create a date vector to fill out empty values\n",
    "    max_date = big_data['OCCUPANCY_DATE'].max()\n",
    "    min_date = big_data['OCCUPANCY_DATE'].min()\n",
    "    date_range = pd.date_range(start=min_date, end=max_date, freq = 'D')\n",
    "    date_df = pd.DataFrame({'OCCUPANCY_DATE': date_range})\n",
    "\n",
    "    print(big_data)\n",
    "    #-------Weather Data-------#\n",
    "\n",
    "    #loading up the links to the weather dataset\n",
    "    for i in range(len(weather_data)):\n",
    "        weather_data[i] = load_csv_to_pandas(weather_data[i])\n",
    "\n",
    "    #Dropping irrelevant columns for weather datasets\n",
    "    for i in range(len(weather_data)):\n",
    "        weather_data[i] = weather_data[i].drop(columns = ['ï»¿\"Longitude (x)\"', 'Latitude (y)', 'Station Name', 'Climate ID', 'Year', 'Month', 'Day', 'Data Quality', 'Max Temp Flag', 'Min Temp Flag', 'Mean Temp Flag', 'Heat Deg Days Flag', 'Cool Deg Days Flag', 'Total Rain (mm)', 'Total Rain Flag', 'Total Snow (cm)', 'Total Snow Flag', 'Total Precip Flag',\n",
    "        'Snow on Grnd Flag', 'Dir of Max Gust (10s deg)', 'Dir of Max Gust Flag', 'Spd of Max Gust (km/h)', 'Spd of Max Gust Flag'])\n",
    "        weather_data[i]['Date/Time'] = weather_data[i]['Date/Time'].astype(str)\n",
    "        weather_data[i]['Date/Time'] = pd.to_datetime(weather_data[i]['Date/Time'])\n",
    "\n",
    "    #Joining the Weather data together\n",
    "    big_weather = weather_data[0]\n",
    "    for i in range(1, len(weather_data)):\n",
    "        big_weather = pd.concat([big_weather, weather_data[i]], ignore_index = True)\n",
    "\n",
    "    #Cut down all data with dates that is bigger than the biggest date and smaller than the smallest date with an output\n",
    "    big_weather = big_weather[big_weather['Date/Time'] <= max_date]\n",
    "    big_weather = big_weather[big_weather['Date/Time'] >= min_date]\n",
    "\n",
    "    #Fill out datasets' entries w no data w 0\n",
    "    big_weather = big_weather.fillna(0)\n",
    "\n",
    "    #Changing non output dataset's date column to 'OCCUPANCY_DATE'\n",
    "    big_weather = big_weather.rename(columns = {'Date/Time': 'OCCUPANCY_DATE'})\n",
    "    print(big_weather)\n",
    "    #-------Inflation Data-------#\n",
    "\n",
    "    #loading up housing data\n",
    "    inflation = load_csv_to_pandas(inflation)\n",
    "\n",
    "    #Dropping irrelevant columns for housing dataset\n",
    "    inflation = inflation.rename(columns = {inflation.columns[0]: 'OCCUPANCY_DATE'})\n",
    "    inflation = inflation.rename(columns = {inflation.columns[1]: 'INFLATION_RATE_CHANGE'})\n",
    "\n",
    "    date_df['OCCUPANCY_DATE'] = pd.to_datetime(date_df['OCCUPANCY_DATE'])\n",
    "    date_df['YEAR_MONTH'] = date_df['OCCUPANCY_DATE'].dt.to_period('M')\n",
    "\n",
    "\n",
    "    if city == \"calgary\":\n",
    "        inflation['OCCUPANCY_DATE'] = pd.to_datetime(inflation['OCCUPANCY_DATE'], format=\"%y-%b\", errors='coerce')\n",
    "\n",
    "        # Prepare unemployment data (monthly values)\n",
    "        inflation['OCCUPANCY_DATE'] = pd.to_datetime(inflation['OCCUPANCY_DATE'])\n",
    "        inflation['YEAR_MONTH'] = inflation['OCCUPANCY_DATE'].dt.to_period('M')\n",
    "\n",
    "        inflation = inflation[\n",
    "            (inflation['OCCUPANCY_DATE'] >= pd.to_datetime(min_date)) &\n",
    "            (inflation['OCCUPANCY_DATE'] <= pd.to_datetime(max_date))\n",
    "        ]\n",
    "\n",
    "        # Merge on the unified OCCUPANCY_DATE column\n",
    "        inflation = pd.merge(date_df, inflation.drop(columns='OCCUPANCY_DATE'), on='YEAR_MONTH', how='left')\n",
    "        \n",
    "\n",
    "    else:\n",
    "        inflation[\"OCCUPANCY_DATE\"] = pd.to_datetime(inflation[\"OCCUPANCY_DATE\"])\n",
    "        inflation = inflation[inflation[\"OCCUPANCY_DATE\"] <= max_date].reset_index(drop=True)\n",
    "        inflation = pd.merge(inflation, date_df, on = 'OCCUPANCY_DATE', how = 'outer')\n",
    "        inflation = inflation.sort_values(by='OCCUPANCY_DATE').reset_index(drop=True)\n",
    "        inflation = inflation.ffill()\n",
    "\n",
    "    inflation = inflation.drop(columns='YEAR_MONTH')\n",
    "\n",
    "    #-------Unemployment Data-------#\n",
    "    \n",
    "    #Loading the unemployment dataset\n",
    "    unemployment = load_csv_to_pandas(unemployment)\n",
    "    \n",
    "    #Analyize Data\n",
    "    unemployment = unemployment.rename(columns = {unemployment.columns[0]:'OCCUPANCY_DATE'})\n",
    "\n",
    "    \n",
    "    if city == \"calgary\":\n",
    "        unemployment = unemployment.rename(columns = {unemployment.columns[1]:'UNEMPLOYMENT_RATE'})\n",
    "    else:\n",
    "        unemployment = unemployment.drop(columns = ['Labour force 7', 'Employment 8', 'Unemployment 9', 'Population 6', \n",
    "                                                'Participation rate 11', 'Employment rate 12'])\n",
    "        unemployment = unemployment.rename(columns = {unemployment.columns[1]:'UNEMPLOYMENT_RATE'})\n",
    "\n",
    "    #     unemployment = unemployment.rename(columns = {\"Population 6\": 'Population'})\n",
    "    #     unemployment['Population'] = unemployment['Population'].str.replace(',', '', regex=False).astype(float)*1000\n",
    "    #     unemployment = unemployment.rename(columns = {\"Unemployment rate 10\": 'UNEMPLOYMENT_RATE'})\n",
    "    #     unemployment = unemployment.rename(columns = {\"Participation rate 11\": 'Employment_Participation_Rate'})\n",
    "    #     unemployment = unemployment.rename(columns = {\"Employment rate 12\": 'Employment_Rate'})\n",
    "\n",
    "    # Convert both date columns to datetime objects representing first day of month\n",
    "    unemployment['OCCUPANCY_DATE'] = pd.to_datetime(unemployment['OCCUPANCY_DATE'], format=\"%y-%b\", errors='coerce')\n",
    "    # date_df['OCCUPANCY_DATE'] = pd.to_datetime(date_df['OCCUPANCY_DATE'])\n",
    "    # date_df['YEAR_MONTH'] = date_df['OCCUPANCY_DATE'].dt.to_period('M')\n",
    "\n",
    "    # Prepare unemployment data (monthly values)\n",
    "    unemployment['OCCUPANCY_DATE'] = pd.to_datetime(unemployment['OCCUPANCY_DATE'])\n",
    "    unemployment['YEAR_MONTH'] = unemployment['OCCUPANCY_DATE'].dt.to_period('M')\n",
    "\n",
    "    unemployment = unemployment[\n",
    "        (unemployment['OCCUPANCY_DATE'] >= pd.to_datetime(min_date)) &\n",
    "        (unemployment['OCCUPANCY_DATE'] <= pd.to_datetime(max_date))\n",
    "    ]\n",
    "\n",
    "    # Merge on the unified OCCUPANCY_DATE column\n",
    "    unemployment = pd.merge(date_df, unemployment.drop(columns='OCCUPANCY_DATE'), on='YEAR_MONTH', how='left')\n",
    "    unemployment = unemployment.drop(columns='YEAR_MONTH')\n",
    "\n",
    "    # unemployment.to_csv(\"unemply.csv\")\n",
    "\n",
    "    #-------CPI Data-------#\n",
    "    \n",
    "    # Load the CPI data\n",
    "    cpi = load_csv_to_pandas(cpi)\n",
    "\n",
    "    # Keep only necessary columns\n",
    "    columns_to_keep = ['ï»¿\"REF_DATE\"', 'Products and product groups', 'VALUE']\n",
    "    cpi = cpi[columns_to_keep]\n",
    "\n",
    "    # Rename for clarity\n",
    "    cpi = cpi.rename(columns={\n",
    "        'ï»¿\"REF_DATE\"': 'OCCUPANCY_DATE',\n",
    "        'Products and product groups': 'CPI_TYPE',\n",
    "        'VALUE': 'CPI_VALUE'\n",
    "    })\n",
    "\n",
    "    cpi = cpi[cpi['CPI_TYPE'] == 'All-items'].reset_index(drop=True)\n",
    "    cpi = cpi.drop(columns = [\"CPI_TYPE\"])\n",
    "\n",
    "    # Convert to datetime (first day of month)\n",
    "    cpi['OCCUPANCY_DATE'] = pd.to_datetime(cpi['OCCUPANCY_DATE'])\n",
    "\n",
    "    # Convert date_df to proper datetime and generate YEAR_MONTH\n",
    "    date_df['OCCUPANCY_DATE'] = pd.to_datetime(date_df['OCCUPANCY_DATE'])\n",
    "    date_df['YEAR_MONTH'] = date_df['OCCUPANCY_DATE'].dt.to_period('M')\n",
    "\n",
    "    cpi['YEAR_MONTH'] = cpi['OCCUPANCY_DATE'].dt.to_period('M')\n",
    "\n",
    "    # Filter CPI based on min/max date\n",
    "    cpi = cpi[\n",
    "        (cpi['OCCUPANCY_DATE'] >= pd.to_datetime(min_date)) &\n",
    "        (cpi['OCCUPANCY_DATE'] <= pd.to_datetime(max_date))\n",
    "    ]\n",
    "\n",
    "    # Merge and broadcast monthly CPI across daily dates\n",
    "    cpi = pd.merge(date_df, cpi.drop(columns='OCCUPANCY_DATE'), on='YEAR_MONTH', how='left')\n",
    "    cpi = cpi.drop(columns='YEAR_MONTH')\n",
    "\n",
    "    #-------Final Data Prep-------#\n",
    "\n",
    "    # Merge the datasets together through date\n",
    "    big_data = pd.merge(big_data, big_weather, on = 'OCCUPANCY_DATE', how = 'left')\n",
    "    big_data = pd.merge(big_data, inflation, on = 'OCCUPANCY_DATE', how = 'left')\n",
    "    big_data = pd.merge(big_data, unemployment, on = 'OCCUPANCY_DATE', how = 'left')\n",
    "    big_data = pd.merge(big_data, cpi, on = 'OCCUPANCY_DATE', how = 'left')\n",
    "\n",
    "    big_data = big_data.sort_values(by='OCCUPANCY_DATE')\n",
    "\n",
    "    ## Need to implement for Calgary data\n",
    "    if city == \"toronto\":\n",
    "        #Placing the bed and room occupancy column last\n",
    "        room_occupancy = big_data.pop('OCCUPANCY_RATE_ROOMS')\n",
    "        bed_occupancy = big_data.pop('OCCUPANCY_RATE_BEDS')\n",
    "        big_data['OCCUPANCY_RATE_BEDS'] = bed_occupancy\n",
    "        big_data['OCCUPANCY_RATE_ROOMS'] = room_occupancy\n",
    "\n",
    "        grouped_data = big_data.groupby('PROGRAM_ID')\n",
    "        shelter_data_frames = {}\n",
    "        for shelter_id, shelter_group in grouped_data:\n",
    "            shelter_data_frames[shelter_id] = shelter_group\n",
    "            shelter_data_frames[shelter_id]['OCCUPANCY_DATE'] = pd.to_datetime(shelter_data_frames[shelter_id]['OCCUPANCY_DATE'])\n",
    "\n",
    "        big_data.reset_index(inplace=True)\n",
    "        big_data = big_data.drop(columns = ['index'])\n",
    "\n",
    "        big_data.to_csv(\"univariate_data.csv\")\n",
    "    return big_data, shelter_data_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0524045",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe, iso_data = loadData(\"toronto\")\n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31afad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_Shelters_Data(df):\n",
    "    df = df.drop(columns = ['ORGANIZATION_NAME', 'SHELTER_GROUP', 'LOCATION_NAME', 'SECTOR', 'ORGANIZATION_ID', 'PROGRAM_MODEL', 'OVERNIGHT_SERVICE_TYPE' ,'LOCATION_ADDRESS', 'LOCATION_POSTAL_CODE', 'PROGRAM_ID', 'CAPACITY_TYPE', 'OCCUPANCY_RATE_BEDS', 'OCCUPANCY_RATE_ROOMS'])\n",
    "    grouped_capacity = df.groupby('OCCUPANCY_DATE')[['CAPACITY_ACTUAL_BED', 'CAPACITY_ACTUAL_ROOM']].sum()\n",
    "    grouped_occupied = df.groupby('OCCUPANCY_DATE')[['OCCUPIED_BEDS', 'OCCUPIED_ROOMS']].sum()\n",
    "\n",
    "    df = df.merge(grouped_capacity, on='OCCUPANCY_DATE', suffixes=('', '_TOTAL_CAPACITY'))\n",
    "    df = df.merge(grouped_occupied, on='OCCUPANCY_DATE', suffixes=('', '_TOTAL_OCCUPIED'))\n",
    "    df = df.drop(columns = ['CAPACITY_ACTUAL_BED', 'CAPACITY_ACTUAL_ROOM', 'OCCUPIED_BEDS', 'OCCUPIED_ROOMS'])\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    df['TOTAL_OCCUPIED'] = df['OCCUPIED_BEDS_TOTAL_OCCUPIED'] + df['OCCUPIED_ROOMS_TOTAL_OCCUPIED']\n",
    "    df['TOTAL_CAPACITY'] = df['CAPACITY_ACTUAL_BED_TOTAL_CAPACITY'] + df['CAPACITY_ACTUAL_ROOM_TOTAL_CAPACITY']\n",
    "    df['OCCUPIED_PERCENTAGE'] = 100 * df['TOTAL_OCCUPIED']/df['TOTAL_CAPACITY']\n",
    "    df = df.drop(columns = ['CAPACITY_ACTUAL_BED_TOTAL_CAPACITY', 'CAPACITY_ACTUAL_ROOM_TOTAL_CAPACITY', 'OCCUPIED_BEDS_TOTAL_OCCUPIED', 'OCCUPIED_ROOMS_TOTAL_OCCUPIED', 'TOTAL_CAPACITY', 'TOTAL_OCCUPIED'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb324792",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merge_Shelters_Data(dataframe)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3c4e32",
   "metadata": {},
   "source": [
    "Convert dataset to dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bda758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaler():\n",
    "    scaler = MinMaxScaler(feature_range = (-1, 1))\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59580a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature_check(df):\n",
    "    df_ = dc(df)\n",
    "    for i in df_.columns:\n",
    "        if df_[i].isna().any():\n",
    "            avg = df_[i].mean()\n",
    "            df_.loc[:, i] = df_[i].fillna(avg)\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9d01d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e25acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_converter(iso_data, scaler, n_past, n_future, train_test_split, batch_size, used_features, shel_group = None):\n",
    "\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "\n",
    "    if shel_group is not None:\n",
    "\n",
    "        one_hot_len = max(shel_group.values()) + 1\n",
    "\n",
    "        num_feat = len(used_features) - 1 + one_hot_len\n",
    "\n",
    "        dfs = []\n",
    "\n",
    "        #Iterate through all useable Shelters\n",
    "        for i in shel_group:\n",
    "\n",
    "            #Getting the df from Iso Data\n",
    "            df = iso_data[int(i)]\n",
    "\n",
    "            #Unifying the df to have the same output column name\n",
    "            if df['OCCUPANCY_RATE_ROOMS'].isna().all():\n",
    "                df = df.rename(columns = {'OCCUPANCY_RATE_BEDS': 'OCCUPIED_PERCENTAGE'})\n",
    "            else:\n",
    "                df = df.rename(columns = {'OCCUPANCY_RATE_ROOMS': 'OCCUPIED_PERCENTAGE'})\n",
    "\n",
    "            df = df[used_features]\n",
    "\n",
    "            for z in range(one_hot_len):\n",
    "                if shel_group[i] == z:\n",
    "                    df['Feature_' + str(z)] = 1\n",
    "                else:\n",
    "                    df['Feature_' + str(z)] = 0\n",
    "\n",
    "            dfs.append(df)\n",
    "\n",
    "            #Concatenating all Dfs together\n",
    "            concatenated_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "            #Isolate the feature columns\n",
    "            iso_col = concatenated_df[['Feature_' + str(i) for i in range(one_hot_len)]]\n",
    "\n",
    "            #Scaled the dfs\n",
    "            scaler = scaler.fit(concatenated_df[used_features[1:]])\n",
    "            np_df = scaler.fit_transform(concatenated_df[used_features[1:]])\n",
    "            df_scaled = pd.DataFrame(np_df, columns=used_features[1:])\n",
    "\n",
    "            #Combined the final df together\n",
    "            np_df = pd.concat([iso_col, df_scaled], axis=1).values\n",
    "\n",
    "    else:\n",
    "        df = dc(iso_data)\n",
    "        df.set_index('OCCUPANCY_DATE', inplace=True)\n",
    "        df = df.astype(float)\n",
    "        scaler = scaler.fit(df)\n",
    "\n",
    "        np_df = scaler.transform(df)\n",
    "\n",
    "        num_feat = len([i for i in df])\n",
    "\n",
    "    #Converting it into a time series\n",
    "    for i in range(n_past, len(np_df) - n_future + 1):\n",
    "        train_x.append(np_df[i - n_past: i, 0:np_df.shape[1]])\n",
    "        train_y.append(np_df[i: i + n_future, - 1])\n",
    "\n",
    "    train_x, train_y = np.array(train_x), np.array(train_y)\n",
    "\n",
    "    split_index = int(len(train_x) * train_test_split)\n",
    "\n",
    "    X_train = train_x[:split_index]\n",
    "    X_test = train_x[split_index:]\n",
    "\n",
    "    Y_train = train_y[:split_index]\n",
    "    Y_test = train_y[split_index:]\n",
    "\n",
    "    X_train_ = X_train.reshape((-1, n_past, num_feat))\n",
    "    X_test_ = X_test.reshape((-1, n_past, num_feat))\n",
    "\n",
    "    X_train = torch.tensor(X_train).float()\n",
    "    Y_train = torch.tensor(Y_train).float()\n",
    "    X_test = torch.tensor(X_test).float()\n",
    "    Y_test = torch.tensor(Y_test).float()\n",
    "\n",
    "    train_Dataset = TimeSeriesDataset(X_train, Y_train)\n",
    "    test_Dataset = TimeSeriesDataset(X_test, Y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_Dataset, batch_size = batch_size, shuffle = True)\n",
    "    test_loader = DataLoader(test_Dataset, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28151b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_stacked_layers, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_stacked_layers = num_stacked_layers\n",
    "\n",
    "        # Define the LSTM layers\n",
    "        self.lstm_layers = nn.ModuleList([nn.LSTM(input_size, hidden_size, batch_first=True)])\n",
    "        for _ in range(1, num_stacked_layers):\n",
    "            self.lstm_layers.append(nn.LSTM(hidden_size, hidden_size, batch_first=True))\n",
    "\n",
    "        # Define the fully connected layers\n",
    "        self.fc_layers = nn.ModuleList([nn.Linear(hidden_size, hidden_size)])\n",
    "        self.fc_layers.append(nn.Linear(hidden_size, output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        out = x\n",
    "\n",
    "        # Initialize hidden state and cell state tensors for each LSTM layer\n",
    "        hidden_states = [torch.zeros(1, batch_size, self.hidden_size) for _ in range(self.num_stacked_layers)]\n",
    "        cell_states = [torch.zeros(1, batch_size, self.hidden_size) for _ in range(self.num_stacked_layers)]\n",
    "\n",
    "        # Pass input through each LSTM layer\n",
    "        for lstm_layer, h0, c0 in zip(self.lstm_layers, hidden_states, cell_states):\n",
    "            out, _ = lstm_layer(out, (h0, c0))\n",
    "\n",
    "        # Apply fully connected layers\n",
    "        out = self.fc_layers[0](out[:, -1, :])  # Apply the first fully connected layer\n",
    "        out = torch.relu(out)  # Apply ReLU activation function\n",
    "        for fc_layer in self.fc_layers[1:]:\n",
    "            out = fc_layer(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2248faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def begin_training(model,num_epochs, train_loader, test_loader, loss, optimizer):\n",
    "\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    average_validation_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        #Training\n",
    "        model.train(True)\n",
    "        running_loss = 0\n",
    "        if epoch % 5 == 0:\n",
    "          print(\"Epoch: \" + str(epoch))\n",
    "\n",
    "        for batch_index, batch in enumerate(train_loader):\n",
    "            x_batch, y_batch = batch[0], batch[1]\n",
    "            output = model(x_batch)\n",
    "            loss_ = loss(output, y_batch)\n",
    "            running_loss += loss_.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "          print(\"Training Loss: \" + str(running_loss))\n",
    "        training_loss.append(running_loss)\n",
    "\n",
    "        #Validating\n",
    "        model.train(False)\n",
    "        vad_loss = 0\n",
    "\n",
    "        for batch_index, batch in enumerate(test_loader):\n",
    "            x_batch, y_batch = batch[0], batch[1]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output    = model(x_batch)\n",
    "                loss_     = loss(output, y_batch)\n",
    "                vad_loss += loss_.item()\n",
    "\n",
    "        validation_loss.append(vad_loss)\n",
    "        avg_loss_across_batches = vad_loss / len(test_loader)\n",
    "        average_validation_loss.append(avg_loss_across_batches)\n",
    "        if epoch % 5 == 0:\n",
    "          print('Val Loss: {0:.3f}'.format(avg_loss_across_batches))\n",
    "          print('***************************************************')\n",
    "          print('\\n')\n",
    "\n",
    "\n",
    "    return model, training_loss, validation_loss, average_validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dbfc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merge_Shelters_Data(dataframe)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321b0597",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_features = ['OCCUPANCY_DATE', 'Max Temp (Â°C)', 'Min Temp (Â°C)', 'Total Precip (mm)', 'INFLATION_RATE_CHANGE', 'UNEMPLOYMENT_RATE', 'CPI_VALUE', 'OCCUPIED_PERCENTAGE']\n",
    "df_one = dc(df[used_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc6941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one = feature_check(df_one)\n",
    "print(df_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3d91fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining model running hyperparameters\n",
    "n_steps = 90\n",
    "n_future = 60\n",
    "batch_size = 16\n",
    "train_test_split = 0.75\n",
    "scaler = get_scaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192dfa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_one, test_loader_one = time_series_converter(df_one, scaler, n_steps, n_future, train_test_split, batch_size, used_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3525c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 100\n",
    "train_test_split = 0.8\n",
    "loss_function = nn.MSELoss()\n",
    "input_size = len(used_features) - 1\n",
    "hidden_size = 120\n",
    "num_stacked_layers = 2\n",
    "output_size = n_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ff5429",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_one = LSTM(input_size, hidden_size, num_stacked_layers, output_size)\n",
    "optimizer = torch.optim.AdamW(model_one.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fa4ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_one, training_loss, valid_loss, avg_valid_loss = begin_training(model_one, num_epochs, train_loader_one, test_loader_one, loss_function, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a20167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_errors(training_loss, valid_loss, avg_valid_loss):\n",
    "\tepochs = [i for i in range(1, len(training_loss) + 1)]\n",
    "\tfig, axs = plt.subplots(1, 3, figsize=(12, 3))\n",
    "\taxs[0].plot(epochs, training_loss)\n",
    "\taxs[1].plot(epochs, valid_loss)\n",
    "\taxs[2].plot(epochs, avg_valid_loss)\n",
    "\n",
    "\taxs[0].set_title('Training Loss')\n",
    "\taxs[0].set_xlabel('Epoch')\n",
    "\taxs[0].set_ylabel('Error')\n",
    "\n",
    "\taxs[1].set_title('Validation Loss')\n",
    "\taxs[1].set_xlabel('Epoch')\n",
    "\taxs[1].set_ylabel('Error')\n",
    "\n",
    "\taxs[2].set_title('Average Validation Loss Per Batch')\n",
    "\taxs[2].set_xlabel('Epoch')\n",
    "\taxs[2].set_ylabel('Error')\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a15abdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors(training_loss, valid_loss, avg_valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9dc9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_date_(model, df, scaler, n_future, future_days = None, one_hot_feature = None):\n",
    "\n",
    "    #Set model training to False\n",
    "    model.train(False)\n",
    "    copy_df = dc(df)\n",
    "\n",
    "    #Case 1: if model predicts multiple days at once\n",
    "    if n_future > 1:\n",
    "\n",
    "        #Build Dateframe for future days\n",
    "        max_date = df['OCCUPANCY_DATE'].max()\n",
    "        date_range = pd.date_range(start=max_date, end=max_date + pd.Timedelta(days=n_future), freq = 'D')\n",
    "        df_new = pd.DataFrame({'OCCUPANCY_DATE': date_range})\n",
    "\n",
    "        #Scaling input Dataframe\n",
    "        copy_df.set_index('OCCUPANCY_DATE', inplace = True)\n",
    "        df_scaled = scaler.fit_transform(copy_df)\n",
    "\n",
    "        if one_hot_feature is not None:\n",
    "            for i in range(one_hot_feature[1]):\n",
    "\n",
    "                if i != one_hot_feature[0]:\n",
    "                    arr = np.zeros((df_scaled.shape[0], 1))\n",
    "                    df_scaled = np.hstack((arr, df_scaled))\n",
    "                else:\n",
    "                    arr = np.ones((df_scaled.shape[0], 1))\n",
    "                    df_scaled = np.hstack((arr, df_scaled))\n",
    "\n",
    "        #Convert data to tensor and passing it into the model to get predicted data and converting it into a panda dataframe before returning it\n",
    "        y = model(torch.tensor(df_scaled).unsqueeze(0).float()).detach().numpy().reshape(-1,1)\n",
    "        y_ = np.repeat(y, copy_df.shape[1], axis = -1)\n",
    "        y_actual = scaler.inverse_transform(y_)[:,-1]\n",
    "        y_inserted = np.insert(y_actual, 0, df['OCCUPIED_PERCENTAGE'].iloc[-1])\n",
    "        df_new['OCCUPIED_PERCENTAGE'] = pd.DataFrame(y_inserted, columns = ['OCCUPIED_PERCENTAGE'])\n",
    "\n",
    "\n",
    "    #Case 2: if model predicts one day at a time; therefore need loop to predict all future_days days.\n",
    "    if n_future == 1 and future_days is not None:\n",
    "\n",
    "        data = torch.tensor(scaler.fit_transform(np.array(copy_df['OCCUPIED_PERCENTAGE']).reshape(-1, 1)).reshape((-1, copy_df.shape[0], 1))).float()\n",
    "        for i in range(future_days):\n",
    "            y = model(data).unsqueeze(0)\n",
    "            data = torch.cat((data, y), dim = 1)\n",
    "        data = scaler.inverse_transform(data.squeeze().detach().numpy().reshape(-1, 1)).flatten()\n",
    "\n",
    "        #Adding a data column to the new data\n",
    "        max_date = copy_df['OCCUPANCY_DATE'].max()\n",
    "        date_range = pd.date_range(start=max_date , end=max_date + pd.Timedelta(days=future_days), freq = 'D')\n",
    "        df_new = pd.DataFrame({'OCCUPANCY_DATE': date_range})\n",
    "\n",
    "        #Getting the newly generated portion of data\n",
    "        new_data = data[-future_days:]\n",
    "        new_data = np.insert(new_data, 0, copy_df['OCCUPIED_PERCENTAGE'].iloc[-1])\n",
    "        new_data_df = pd.DataFrame(new_data, columns = ['OCCUPIED_PERCENTAGE'])\n",
    "\n",
    "        #Combined\n",
    "        df_new['OCCUPIED_PERCENTAGE'] = new_data_df\n",
    "\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb60ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_general(model, df, n_future, scaler, test_check = False, future_days = None):\n",
    "\n",
    "  #Deep copying the dataframe\n",
    "  copy_df = dc(df)\n",
    "\n",
    "  #If test check, move the data back by n_future days inorder to view model's performance\n",
    "  if test_check:\n",
    "\n",
    "    if future_days is None:\n",
    "      use_date = max(copy_df['OCCUPANCY_DATE']) - pd.Timedelta(days = n_future)\n",
    "      copy_df = copy_df[copy_df['OCCUPANCY_DATE'] <= use_date]\n",
    "\n",
    "    elif future_days is not None:\n",
    "      use_date = max(copy_df['OCCUPANCY_DATE']) - pd.Timedelta(days = future_days)\n",
    "      copy_df = copy_df[copy_df['OCCUPANCY_DATE'] <= use_date]\n",
    "\n",
    "  #Getting the inferred data\n",
    "  data_frame = infer_date_(model, copy_df, scaler,n_future, future_days)\n",
    "\n",
    "  plt.plot(df['OCCUPANCY_DATE'], df['OCCUPIED_PERCENTAGE'], label='Actual')\n",
    "  plt.plot(data_frame['OCCUPANCY_DATE'], data_frame['OCCUPIED_PERCENTAGE'], label='Predicted')\n",
    "  plt.title('All Shelters Occupancy Rates')\n",
    "  plt.xlabel('Date')\n",
    "  plt.ylabel('Occupied Percentage (%)')\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a0b2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_check = True\n",
    "df = df[used_features]\n",
    "plot_general(model_one, df, n_future, scaler, test_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f38cad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_shelters(model, iso_data, n_future, num_sq, scaler, used_features, test_check = False, future_days = None):\n",
    "\n",
    "\trandom_keys = random.sample(list(iso_data), num_sq ** 2)\n",
    "\tfig, axs = plt.subplots(num_sq, num_sq, figsize=(11, 8))\n",
    "\tfor i in range(num_sq):\n",
    "\t\tfor j in range(num_sq):\n",
    "\n",
    "\t\t\tshelter_index = random_keys[i * num_sq + j]  # Calculate the index from the 1D array\n",
    "\n",
    "\t\t\t#Preprocess the iso by copying it into local variables to avoid changing the iso_data itself\n",
    "\t\t\tdf_use = dc(iso_data[shelter_index])\n",
    "\t\t\tmax_date = df_use['OCCUPANCY_DATE'].max()\n",
    "\n",
    "\t\t\tif df_use['OCCUPANCY_RATE_ROOMS'].isna().all():\n",
    "\t\t\t\tdf_use = df_use.rename(columns = {'OCCUPANCY_RATE_BEDS': 'OCCUPIED_PERCENTAGE'})\n",
    "\t\t\telse:\n",
    "\t\t\t\tdf_use = df_use.rename(columns = {'OCCUPANCY_RATE_ROOMS': 'OCCUPIED_PERCENTAGE'})\n",
    "\n",
    "\t\t\tdf_infer = df_use[used_features]\n",
    "\n",
    "\t\t\tdf_infer = feature_check(df_infer)\n",
    "\n",
    "\t\t\t#If test check, move the data back by n_future days inorder to view model's performance\n",
    "\t\t\tif test_check:\n",
    "\n",
    "\t\t\t\tif future_days is None:\n",
    "\t\t\t\t\tuse_date = max(df_use['OCCUPANCY_DATE']) - pd.Timedelta(days = n_future)\n",
    "\t\t\t\t\tdf_infer = df_infer[df_infer['OCCUPANCY_DATE'] <= use_date]\n",
    "\n",
    "\t\t\t\telif future_days is not None:\n",
    "\t\t\t\t\tuse_date = max(df_use['OCCUPANCY_DATE']) - pd.Timedelta(days = future_days)\n",
    "\t\t\t\t\tdf_infer = df_infer[df_infer['OCCUPANCY_DATE'] <= use_date]\n",
    "\n",
    "\t\t\ttry:\n",
    "\t\t\t\tdf = infer_date_(model, df_infer, scaler, n_future, future_days)\n",
    "\n",
    "\t\t\t\taxs[i, j].plot(df_use['OCCUPANCY_DATE'], df_use['OCCUPIED_PERCENTAGE'])\n",
    "\t\t\t\taxs[i, j].plot(df['OCCUPANCY_DATE'], df['OCCUPIED_PERCENTAGE'])\n",
    "\n",
    "\t\t\t\t#Labeling\n",
    "\t\t\t\taxs[i, j].set_title(f'Shelter {shelter_index}')\n",
    "\t\t\t\taxs[i, j].set_xlabel('Date')\n",
    "\t\t\t\taxs[i, j].set_ylabel('Occupied Percentage (%)')\n",
    "\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tprint(\"Error: \" + str(e))\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381cdfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in iso_data.copy():\n",
    "  if iso_data[i].shape[0] <= n_steps - 1:\n",
    "    del iso_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556d2de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_check = True\n",
    "num_sq = 3\n",
    "plot_random_shelters(model_one, iso_data, n_future, num_sq, scaler, used_features, test_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e569f541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce iso_data to just the 5 shelters being used for testing\n",
    "shelters_test = [11794, 11831, 12252, 12254, 12274]\n",
    "test_data = iso_data.copy()\n",
    "for i in iso_data.copy():\n",
    "  if i not in shelters_test:\n",
    "    del test_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f611be82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiplying_factor(pred, shel ,adj_factors):\n",
    "  mult = 1\n",
    "  for i in adj_factors:\n",
    "    adj =  (100 + float(adj_factors[i][adj_factors[i][i] == shel[i].iloc[0]]['PERCENT_DIFF'].iloc[0])) / 100\n",
    "    mult *= adj\n",
    "  return mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d8c9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(model, iso_data, used_features, shel_grouping = None, adj_factors = None):\n",
    "  #Initialize the Loss variables\n",
    "  total_shel = 0\n",
    "  loss_ = 0\n",
    "\n",
    "  total_shel_ = 0\n",
    "  loss_sq = 0\n",
    "\n",
    "  for i in iso_data:\n",
    "\n",
    "    if len(iso_data[i]) > 60:\n",
    "      df = iso_data[i]\n",
    "\n",
    "      #Rename the columns\n",
    "      if df['OCCUPANCY_RATE_ROOMS'].isna().all():\n",
    "        df = df.rename(columns = {'OCCUPANCY_RATE_BEDS': 'OCCUPIED_PERCENTAGE'})\n",
    "      else:\n",
    "        df = df.rename(columns = {'OCCUPANCY_RATE_ROOMS': 'OCCUPIED_PERCENTAGE'})\n",
    "\n",
    "\n",
    "      df_mult = df.copy()\n",
    "\n",
    "      #Selecting on used features\n",
    "      df = df[used_features]\n",
    "\n",
    "      df = feature_check(df)\n",
    "\n",
    "      #Move the dates back by 60 days\n",
    "      df_use_infer = dc(df)\n",
    "      use_date = max(df_use_infer['OCCUPANCY_DATE']) - pd.Timedelta(days = 60)\n",
    "      df_use_infer = df_use_infer[df_use_infer['OCCUPANCY_DATE'] <= use_date]\n",
    "\n",
    "      #Inferring Data\n",
    "      if shel_grouping is None:\n",
    "        df_infer = infer_date_(model, df_use_infer, scaler, n_future, future_days = 60, one_hot_feature = None)\n",
    "        df_infer_date = min(df_infer['OCCUPANCY_DATE'])\n",
    "        df_infer = df_infer[df_infer['OCCUPANCY_DATE'] > df_infer_date]\n",
    "      else:\n",
    "        df_infer = infer_date_(model, df_use_infer, scaler, n_future, future_days = 60, one_hot_feature = [shel_grouping[i], max(shel_grouping.values()) + 1])\n",
    "        df_infer_date = min(df_infer['OCCUPANCY_DATE'])\n",
    "        df_infer = df_infer[df_infer['OCCUPANCY_DATE'] > df_infer_date]\n",
    "\n",
    "      #Obtain and apply the multiplying factor\n",
    "      if adj_factors is not None:\n",
    "        mult = multiplying_factor(df_infer, df_mult, adj_factors)\n",
    "        df_infer['OCCUPIED_PERCENTAGE'] *= mult\n",
    "\n",
    "      #data used for loss calculations\n",
    "      df_loss = dc(df)\n",
    "      df_loss = df_loss[df_loss['OCCUPANCY_DATE'] > use_date]\n",
    "\n",
    "      #Calculating Loss\n",
    "      df_np = df_loss['OCCUPIED_PERCENTAGE'].values\n",
    "      df_np_infer = df_infer['OCCUPIED_PERCENTAGE'].values\n",
    "\n",
    "      temp_loss = 0\n",
    "      temp_loss_sq = 0\n",
    "      for i in range(len(df_np)):\n",
    "        temp_loss += abs(df_np[i] - df_np_infer[i])\n",
    "        temp_loss_sq += abs(df_np[i] - df_np_infer[i]) ** 2\n",
    "      if temp_loss > 0:\n",
    "        total_shel += 1\n",
    "        loss_ += temp_loss\n",
    "      if temp_loss_sq > 0:\n",
    "        total_shel_ += 1\n",
    "        loss_sq += temp_loss_sq\n",
    "      loss_ /= 60\n",
    "      loss_sq /= 60\n",
    "      loss_sq = loss_sq ** (1/2)\n",
    "\n",
    "\n",
    "  avg_loss = loss_/total_shel\n",
    "  avg_loss_sq = loss_sq/total_shel_\n",
    "  return avg_loss, avg_loss_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5590949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, loss_sq = calc_loss(model_one, test_data.copy(), used_features)\n",
    "\n",
    "# Mean Absolute Error\n",
    "print(\"MAE Loss: \" + str(loss))\n",
    "# Root Mean Squared Error\n",
    "print(\"MRSE Loss: \" + str(loss_sq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018dac03",
   "metadata": {},
   "source": [
    "Second Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83dfe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_features = ['OCCUPANCY_DATE', 'Max Temp (Â°C)', 'Min Temp (Â°C)', 'Total Precip (mm)', 'OCCUPIED_PERCENTAGE']\n",
    "df_one = dc(df[used_features])\n",
    "df_one = feature_check(df_one)\n",
    "print(df_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327b487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining model running hyperparameters\n",
    "n_steps = 90\n",
    "n_future = 60\n",
    "batch_size = 16\n",
    "train_test_split = 0.75\n",
    "scaler = get_scaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ad95c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_one, test_loader_one = time_series_converter(df_one, scaler, n_steps, n_future, train_test_split, batch_size, used_features)\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 100\n",
    "train_test_split = 0.8\n",
    "loss_function = nn.MSELoss()\n",
    "input_size = len(used_features) - 1\n",
    "hidden_size = 120\n",
    "num_stacked_layers = 2\n",
    "output_size = n_future\n",
    "\n",
    "model_one = LSTM(input_size, hidden_size, num_stacked_layers, output_size)\n",
    "optimizer = torch.optim.AdamW(model_one.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbae81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_one, training_loss, valid_loss, avg_valid_loss = begin_training(model_one, num_epochs, train_loader_one, test_loader_one, loss_function, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9822bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors(training_loss, valid_loss, avg_valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b826a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_check = True\n",
    "df = df[used_features]\n",
    "plot_general(model_one, df, n_future, scaler, test_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c432979",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_check = True\n",
    "num_sq = 3\n",
    "plot_random_shelters(model_one, iso_data, n_future, num_sq, scaler, used_features, test_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fb2f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, loss_sq = calc_loss(model_one, test_data.copy(), used_features)\n",
    "\n",
    "# Mean Absolute Error\n",
    "print(\"MAE Loss: \" + str(loss))\n",
    "# Root Mean Squared Error\n",
    "print(\"MRSE Loss: \" + str(loss_sq))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
